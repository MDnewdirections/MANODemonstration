{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Jupyter Notebook is  great environment for data science practice and instruction.\n",
    "### The notebook is made up of cells in which the user can writer code or markdown languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This block of text is actually a cell written in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running this code cell by pressing shift + enter\n",
    "print(\"Hello, world!\")\n",
    "print('This block of text is the printed product of a cell written in code!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can determine a cell's code/markdown output above by clicking on the \"Cell\" tab.\n",
    "### Then select cell type and choose an option. A cell's default type is code. To give a better \n",
    "### idea about what a cell is I'll leave an empty cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New cells can be created in using the \"Insert\" tab above. Alternatively, a new cell can be created using keyboard commands. A new cell can be created by selecting a cell (indicated by a blue outline) and typing a or b to create a new cell above or below the selected cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Edit\" tab can be used to copy, move, or delete cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cell can be run from the \"Cell\" tab or by simply bressing shift + enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tihs markdown cell is ful off tyyppo corrct it by double clicking on the text and pressing shift + enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You might have noticed the play button on the top tool bar. That can be used for running cells. However, shift + enter is much handier. The square stop button is very important. This can be used to stop a cell from running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cells can be utilized to enter expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_name = input(\"What is your name?: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = 0\n",
    "while spam < 5:\n",
    "    print(\"Spam is less than 5!\")\n",
    "    spam += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in range(5):\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numbers(number1, number2):\n",
    "    result = number1 + number2\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_numbers(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_numbers(number1, number2):\n",
    "    result = number1 - number2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_numbers(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation = subtract_numbers(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebooks are great for learning data scientists because they offer a ton of resources and explanations about the many python tools you'll encounter.\n",
    "\n",
    "### One such resource can be accessed by simply typing a question mark at the end of an object. Let's try this out by learning a bit about the object from the numpy library called arange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the cell has produced detailed documentation about \"arange\", the parameters it takes, and some examples of its use. Let's test out the parameters and usage for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we've demonstrated the simple layout and the presence of reference aids built into the notebook, let's demonstrate some of the useful open-source libraries that we can access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas is the most important library to master as you begin learning about data analysis in python and jupyter notebooks. Similar to how we imported numpy above, let's import pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we import pandas we use \"as\" to assign the pandas library to an abbreviated name \"pd\". This helps to keep our code short & sweet as we utilize pandas many tools. Let's use pandas to conduct some analysis based on data from an csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we'll identify the file containing the data we want to analyze by providing the filepath. We provide the filepath as a string and assign it the name 'filepath'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the filepath of the csv file you'd like to analyze between the quotation marks.\n",
    "filepath = r\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will use pandas to \"read\" the data in the file and format it as a dataframe (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at our data now that its a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same action can be taken with an excel file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the corresponding excel filepath between the quotation marks\n",
    "filepath = r\"\"\n",
    "df = pd.read_excel(filepath)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No matter how your data is stored, there is a way to retrieve it as a dataframe in python with pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When calling a dataframe as we did, we only get a partial view of our data. We can observe the first and last  5 rows and the first and last 10 columns. In order to get a better sense of the data that we are analyzing let's see an overview of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using .info() we get a display of all the columns, the count of their non-null values, and a summary of the type or format of the data held. In this example, 'object' indicates text or 'string' values, 'int64' indicates whole numerical values such as (1, 2, 3), and 'float64' indication numercial values with decimals such as (1.0, 2.0, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a closer look at our dataframe specifically the top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .head() will show you the top of your dataframe. You can also specify how many rows you'd like to observe inside the parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a look at our dataframe its difficult to identify how the data is being sorted. Let's say we're only interested in the top HR hitters and want to sort our dataframe by that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('HR').head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like our data is assorted in ascending order, let's change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('HR', ascending= False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we know the top 15 HR hitters, let's take a look at the bottom 15 using .tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('HR', ascending = False).tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get some more information about HR's by conducting descriptive analysis on the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HR'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .describe() gives us a basic descriptive analysis of the HR category. Let's add some more columns to find out more about each statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we'll create a list of all the columns we'd like to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['HR', 'R', 'RBI', 'AVG', 'OBP', 'WAR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we'll apply them to the dataframe the same way we did with 'HR'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data might be more appealing to look at with the column and row indexes swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas makes it easy to conduct advanced analytics. For instance, let's take a look at the correlation between each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The closer the number is to 1, the higher correlation between each category. The closer the number is to -1, the lesser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing correlations between batting statistics over one year is nice, but larger sample sizes tend to tell a more accurate story when it comes to statistical analyses. So let's make a bigger sample size, by including statistics from the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the 2020 leaderboard file between the quotation marks.\n",
    "filepath = r\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the data with .info() we see that the dataframes are nearly identical. The only major difference being that this data frame has 142 rows of data while the previous one had only 132. We're going to merge these two dataframes together, but we'd also like to be able to tell them apart once they are merged. Let's add a 'Year' column to each data frame to distinguish the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = 2021\n",
    "df2['Year'] = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many ways to aggregate data in pandas, a simple way to aggregate two dataframes with identical columns and data types is to concatenate them. First, make a list of the dataframes you'd like to concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df, df2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, use pd.concat to concatenate the listed dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concat_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a sample size of 274 entries to analyze. Let's see if the descriptive stats and correlations we observed before remain consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df[cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our statistics regarding HR, R, and RBI have all noticabely dropped, and there's a markedly higher standard deviation in most categories. However, let's see if correlation between categories remains consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point let's highlight how much easier it is to compare these outcomes using visualizations. Below we'll take a look at the two sets of correlations we have using a heatmap built using matplotlib and seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "ax = sns.heatmap(df[cols].corr(),\n",
    "                cmap = sns.cubehelix_palette(20,light=0.95, \n",
    "                                               dark = 0.15))\n",
    "ax.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "ax = sns.heatmap(concat_df[cols].corr(),\n",
    "                cmap = sns.cubehelix_palette(20,light=0.95, \n",
    "                                               dark = 0.15))\n",
    "ax.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including the pandemic shortened season to our analysis we see a distinctly higher correlation between HR, R, and RBI and a miniscule correlation between these scoring categories and AVG/OBP. From these charts we can speculate on reasons why there's such an extreme shift in these patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's no shortage of methods by which we can compare information using matplot lib and seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(concat_df[cols],\n",
    "            plot_kws={'alpha':0.6},\n",
    "             diag_kws={'bins':30})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we've explored what we can do with python let's create a function that will perform the task that we will want to use again in the future by creating a function that conducts this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The heatmap seemed like the simplest way to identify correlations among batting statistics so lets create a function that takes any number of dataframes and returns a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batting_correlation_heatmap(*dataframes):\n",
    "    \"\"\"A function that produces a heatmap identifying correlations\n",
    "    between various batting statistics.\"\"\"\n",
    "    # produce a list of dataframes\n",
    "    df_list = list(dataframes)\n",
    "    # concatenate dataframes in list\n",
    "    concat_df = pd.concat(df_list)\n",
    "    # identify columns to analyze\n",
    "    cols = ['HR', 'R', 'RBI', 'AVG', 'OBP', 'WAR']\n",
    "    # print a heatmap of the correlations\n",
    "    %matplotlib inline\n",
    "    ax = sns.heatmap(concat_df[columns].corr(),\n",
    "                    cmap = sns.cubehelix_palette(20,light=0.95, \n",
    "                                                   dark = 0.15))\n",
    "    return ax.xaxis.tick_top()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"2020\")\n",
    "batting_correlation_heatmap(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2021\")\n",
    "batting_correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Combined\")\n",
    "batting_correlation_heatmap(df, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That concludes our introduction to conducting data analysis with python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
